# -*- coding: utf-8 -*-
"""landslide_ajtt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13rh9M4uKgRVyXWxjIudF9pvcIT8ZuHfx

# setup
"""

!pip install tensorflow #latest

!pip install matplotlib #visualisation of image

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import h5py #reading
import glob #iteration
import matplotlib.pyplot as plt
# %matplotlib inline
import tensorflow as tf

from google.colab import auth
auth.authenticate_user()

from google.colab import drive
drive.mount('/content/drive')
#to import google drive files

import os
os.chdir("/content/gdrive/MyDrive/DL/landslide4Sense")  #upload directory path here

"""# Dataset testing

"""

# Testing the dataset
path_single = "data/img/image_2000.h5" #provide path after the cell 5
path_single_mask = "data/mask/mask_2000.h5"

f_data = np.zeros((1, 128, 128, 3))
with h5py.File(path_single) as hdf:
    ls = list(hdf.keys())
    print("ls", ls)
    data = np.array(hdf.get('img'))
    print("input data shape:", data.shape)
    plt.imshow(data[1, :, :, 3:0:-1])

# data_red = data[:, :, 3]
# data_green = data[:, :, 2]
# data_blue = data[:, :, 1]
# data_nir = data[:, :, 7]
# data_rgb = data[:, :, 3:0:-1]
# data_ndvi = np.divide(data_nir - data_red, np.add(data_nir, data_red))
# f_data[0, :, :, 0] = data_ndvi
# f_data[0, :, :, 1] = data[:, :, 12]
# f_data[0, :, :, 2] = data[:, :, 13]

# print("data ndvi shape ", data_ndvi.shape, "f_data shape: ", f_data.shape)
# plt.imshow(data_ndvi)

with h5py.File(path_single_mask) as hdf: #to read h5 file
    ls = list(hdf.keys())
    print("ls", ls)
    data = np.array(hdf.get("mask"))
    print("input data shape:", data.shape)
    plt.imshow(data)



"""# dataset usage

"""

path_single = r"data/img/image_10.h5"
path_single_mask = r"data/mask/mask_1.h5"
TRAIN_PATH = r"data/img/*.h5"
TRAIN_MASK = r"data/mask/*.h5"

TRAIN_XX = np.zeros((3799, 128, 128, 6))
TRAIN_YY = np.zeros((3799, 128, 128, 1))
all_train = sorted(glob.glob(TRAIN_PATH))
all_mask = sorted(glob.glob(TRAIN_MASK))

"""# train with rgb,ndvi,dem and slope"""

#testing for google colab GPU
import tensorflow as tf
tf.test.gpu_device_name()

for i, (img, mask) in enumerate(zip(all_train, all_mask)):
    print(i, img, mask)
    with h5py.File(img) as hdf:
        ls = list(hdf.keys())
        data = np.array(hdf.get('img'))

    # assign 0 for the nan value
    data[np.isnan(data)] = 0.000001

    # to normalize the data
    mid_rgb = data[:, :, 1:4].max() / 2.0
    mid_slope = data[:, :, 12].max() / 2.0
    mid_elevation = data[:, :, 13].max() / 2.0

    # ndvi calculation
    data_red = data[:, :, 3]
    data_nir = data[:, :, 7]
    data_ndvi = np.divide(data_nir - data_red, np.add(data_nir, data_red))

    # final array
    TRAIN_XX[i, :, :, 0] = 1 - data[:, :, 3] / mid_rgb        # RED
    TRAIN_XX[i, :, :, 1] = 1 - data[:, :, 2] / mid_rgb        # GREEN
    TRAIN_XX[i, :, :, 2] = 1 - data[:, :, 1] / mid_rgb        # BLUE
    TRAIN_XX[i, :, :, 3] = data_ndvi                          # NDVI
    TRAIN_XX[i, :, :, 4] = 1 - data[:, :, 12] / mid_slope     # SLOPE
    TRAIN_XX[i, :, :, 5] = 1 - data[:, :, 13] / mid_elevation # ELEVATION

"""# testing min and max"""

# TRAIN_XX_n = TRAIN_XX / TRAIN_XX.max()

TRAIN_XX[np.isnan(TRAIN_XX)] = 0.000001
print(TRAIN_XX.min(), TRAIN_XX.max(), TRAIN_YY.min(), TRAIN_YY.max())

"""# custom loss function (die losse)"""

def dice_loss(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.math.sigmoid(y_pred)
    numerator = 2 * tf.reduce_sum(y_true * y_pred)
    denominator = tf.reduce_sum(y_true + y_pred)

    return 1 - numerator / denominator

"""# visualisation of training data"""

img = 234
fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(15, 10))

ax1.set_title("RGB image")
ax2.set_title("NDVI")
ax3.set_title("Slope")
ax4.set_title("Elevation")
ax5.set_title("Mask")

ax1.imshow(TRAIN_XX[img, :, :, 0:3])
ax2.imshow(TRAIN_XX[img, :, :, 3])
ax3.imshow(TRAIN_XX[img, :, :, 4])
ax4.imshow(TRAIN_XX[img, :, :, 5])
ax5.imshow(TRAIN_YY[img, :, :, 0])

"""# validation split"""

from sklearn.model_selection import train_test_split

# Split the data
x_train, x_valid, y_train, y_valid = train_test_split(TRAIN_XX, TRAIN_YY, test_size=0.2, shuffle=True)

img = 1545
fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(15, 10))

ax1.set_title("RGB image")
ax2.set_title("NDVI")
ax3.set_title("Slope")
ax4.set_title("Elevation")
ax5.set_title("Mask")

ax1.imshow(x_train[img, :, :, 0:3])
ax2.imshow(x_train[img, :, :, 3])
ax3.imshow(x_train[img, :, :, 4])
ax4.imshow(x_train[img, :, :, 5])
ax5.imshow(y_train[img, :, :, 0])

x_train.shape, y_train.shape

# to release some memory, delete the unnecessary variable
del TRAIN_XX
del TRAIN_YY
del all_train
del all_mask

img = 1545
fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15, 10))

ax1.set_title("RGB image")
ax2.set_title("NDVI")
ax3.set_title("SLOPE")
ax4.set_title("Mask")

ax1.imshow(x_train[img, :, :, 0:3])
ax2.imshow(x_train[img, :, :, 3])
ax3.imshow(x_train[img, :, :, 4])
ax4.imshow(y_train[img, :, :, 0])

"""# unet model

"""

from utils import recall_m, precision_m, f1_m

def unet_model(IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS):
    inputs = tf.keras.layers.Input((IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS))

    # Converted inputs to floating
    # s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)

    # Contraction path (Encoder)
    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)
    c1 = tf.keras.layers.Dropout(0.1)(c1)
    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)
    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)

    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)
    c2 = tf.keras.layers.Dropout(0.1)(c2)
    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)
    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)

    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)
    c3 = tf.keras.layers.Dropout(0.2)(c3)
    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)
    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)

    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)
    c4 = tf.keras.layers.Dropout(0.2)(c4)
    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)
    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)

    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)
    c5 = tf.keras.layers.Dropout(0.3)(c5)
    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)
# Expansive path

    u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)
    u6 = tf.keras.layers.concatenate([u6, c4])
    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)
    c6 = tf.keras.layers.Dropout(0.2)(c6)
    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)

    u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)
    u7 = tf.keras.layers.concatenate([u7, c3])
    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)
    c7 = tf.keras.layers.Dropout(0.2)(c7)
    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)

    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)
    u8 = tf.keras.layers.concatenate([u8, c2])
    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)
    c8 = tf.keras.layers.Dropout(0.1)(c8)
    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)

    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)
    c9 = tf.keras.layers.Dropout(0.1)(c9)
    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)

    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)

    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_m, precision_m, recall_m])

    return model

model = unet_model(128, 128, 6)
# model.summary()

checkpointer = tf.keras.callbacks.ModelCheckpoint("best_model.h5", monitor="val_f1_m", verbose=1, save_best_only=True, mode="max")
# earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_m', patience=10, verbose=1, mode='max')

callbacks = [
    # earlyStopping,
    checkpointer
]

history = model.fit(x_train, y_train, batch_size=16,
                    epochs=100,
                    verbose=2,
                    validation_data=(x_valid, y_valid),
                    callbacks=callbacks)

model.save("model_save.h5")

loss, accuracy, f1_score, precision, recall = model.evaluate(x_valid, y_valid, verbose=0)
print(loss, accuracy, f1_score, precision, recall)

"""# prediction

"""

fig, ((ax11, ax12), (ax13, ax14)) = plt.subplots(2, 2, figsize=(20, 15))

# Loss plot
ax11.plot(history.history['loss'])
ax11.plot(history.history['val_loss'])
ax11.title.set_text('Unet model loss')
ax11.set_ylabel('loss')
ax11.set_xlabel('epoch')
ax11.legend(['train', 'validation'], loc='upper left')

# Precision plot
ax12.plot(history.history['precision_m'])
ax12.plot(history.history['val_precision_m'])
ax12.set_title('Unet model precision')
ax12.set_ylabel('precision')
ax12.set_xlabel('epoch')
ax12.legend(['train', 'validation
fig, ((ax11, ax12), (ax13, ax14)) = plt.subplots(2, 2, figsize=(20, 15))

# Loss plot
ax11.plot(history.history['loss'])
ax11.plot(history.history['val_loss'])
ax11.title.set_text('Unet model loss')
ax11.set_ylabel('loss')
ax11.set_xlabel('epoch')
ax11.legend(['train', 'validation'], loc='upper left')


prediction'], loc='upper left')

# Recall plot
ax13.plot(history.history['recall_m'])
ax13.plot(history.history['val_recall_m'])
ax13.set_title('Unet model recall')
ax13.set_ylabel('recall')
ax13.set_xlabel('epoch')
ax13.legend(['train', 'validation'], loc='upper left')

# F1 Score plot
ax14.plot(history.history['f1_m'])
ax14.plot(history.history['val_f1_m'])
ax14.set_title('Unet model f1')
ax14.set_ylabel('f1')
ax14.set_xlabel('epoch')
ax14.legend(['trai
# Recall plot
ax13.plot(history.history['recall_m'])
ax13.plot(history.history['val_recall_m'])
ax13.set_title('Unet model recall')
ax13.set_ylabel('recall')
ax13.set_xlabel('epoch')
ax13.legend(['train', 'validation'], loc='upper left')

# F1 Score plot
ax14.plot(history.history['f1_m'])

predictionn', 'validation'], loc='upper left')
#output will be a graph

threshold = 0.5
pred_img = model.predict(x_valid)
pred_img = (pred_img > threshold).astype(np.uint8)

img = 150
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 10))

ax1.imshow(pred_img[img, :, :, 0])
ax1.set_title("Predictions")

ax2.imshow(y_valid[img, :, :, 0])
ax2.set_title("Label")

ax3.imshow(x_valid[img, :, :, 0:3])
ax3.set_title("Training Image")

"""# Validation data

"""

validation_url = r"/content/drive/MyDrive/DL/landslide4Sense/data/validation/img/*.h5"
img_val = sorted(glob.glob(validation_url))

VAL_XX = np.zeros((245, 128, 128, 6))  # 245 samples, 128x128 size, 6 channels
mask_name = []

for i_img, img in enumerate(img_val):
    mask_name.append(img.split('/')[-1].replace('image', 'mask'))
    print(i_img)
    with h5py.File(img as hdf:
        ls = list(hdf.keys())
        data = np.array(hdf.get('img'))

    # assign 0 for the nan value
    data[np.isnan(data)] = 0.000001

    # to normalize the data
    mid_rgb = data[:, :, 1:4].max() / 2.0
    mid_nir = data[:, :, 4].max() / 2.0
    mid_elevation = data[:, :, 13].max() / 2.0

    # NDVI calculation
    data_red = data[:, :, 3]
    data_nir = data[:, :, 4]
    data_ndvi = np.divide(data_nir - data_red, np.add(data_nir, data_red))

    # final array
    VAL_XX[i_img, :, :, 0] = data[:, :, 1] - mid_rgb   # RED
    VAL_XX[i_img, :, :, 1] = data[:, :, 2] - mid_rgb   # GREEN
    VAL_XX[i_img, :, :, 2] = data[:, :, 3] - mid_rgb   # BLUE
    VAL_XX[i_img, :, :, 3] = data_ndvi                     # NDVI
    VAL_XX[i_img, :, :, 4] = data[:, :, 14] / mid_slope     # SLOPE
    VAL_XX[i_img, :, :, 5] = data[:, :, 13] / mid_elevation # ELEVATION

"""# Prediction for validation data"""

threshold = 0.5
pred_img = model.predict(VAL_XX)
pred_img = (pred_img > threshold).astype(np.uint8)
pred_img.shape

"""# visualization of validation image"""

img = 150
fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,10))
ax1.imshow(pred_img[:,:,0])
ax1.set_title("Predictions")
ax2.imshow(val_xx[img, :, :, 0:3])
ax2.set_title("Training Image")

write_directory = r"/content/gdrive/MyDrive/DL/Landslide4Sense/data/validation/mask"
    for i, name in enumerate(mask_name):
        hsf = h5py.File(write_directory + "/" + name, 'w')
        # change the dimension of prediction to (n, 128, 128)
        pred_mask = pred_img[i, :, :, 0]
        # write to the directory
        hsf.create_dataset('mask', data=pred_mask)
        hsf.close()
